import json
import re
import yaml
import glob
import os
import requests
from collections import Counter
from enum import Enum
from dataclasses import dataclass, field
from typing import List, Dict, Optional, Set

# kiwipiepy í™•ì¸
try:
    from kiwipiepy import Kiwi
except ImportError:
    print("âŒ 'kiwipiepy' ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ ì—†ìŠµë‹ˆë‹¤. 'pip install kiwipiepy'ë¥¼ ì‹¤í–‰í•´ì£¼ì„¸ìš”.")
    exit(1)

# =========================================================
# 0. Configuration & Static Data
# =========================================================

DEFAULT_MRS_CONFIG = """
mrs_schema:
  mrs_only_types:
    type_order: [T6_VerificationCentric, T5_ConstraintCentric, T4_WhenCentric, T3_HowTypeCentric, T2_WhatCentric, T1_WhyCentric]
    types:
      T1_WhyCentric: { match: { all: [{slot: Why, state_in: [OK]}] } }
      T2_WhatCentric: { match: { all: [{slot: What, state_in: [OK]}, {slot: Anchor, state_in: [OK]}] } }
      T3_HowTypeCentric: { match: { all: [{slot: HowType, state_in: [OK]}] } }
      T4_WhenCentric: { match: { all: [{slot: When, state_in: [OK]}] } }
      T5_ConstraintCentric: { match: { all: [{slot: Constraints, state_in: [OK]}] } }
      T6_VerificationCentric: { match: { all: [{slot: Verification, state_in: [OK]}, {slot: AcceptanceCriteria, state_in: [OK]}] } }
"""

# [ìœ„ê³„ ê·œì¹™] ê´€ê³„ ì •ì˜
# ë…¼ë¦¬: Right(í›„ë°˜ë¶€)ê°€ ì¡´ì¬í•˜ë ¤ë©´ Left(ì „ë°˜ë¶€)ê°€ ë°˜ë“œì‹œ ì¡´ì¬í•´ì•¼ í•¨.
#      (Leftê°€ ì—†ìœ¼ë©´ RightëŠ” ë¬´íš¨/Pruning ëŒ€ìƒ)
HIERARCHY_RELATIONS = [
    {"left": "Constraints", "right": "When", "rel": "requires"},
    {"left": "When", "right": "Constraints", "rel": "refines"},
    {"left": "When", "right": "What", "rel": "triggers"},
    {"left": "Why", "right": "What", "rel": "justifies"},
    {"left": "Constraints", "right": "What", "rel": "qualifies"},
    {"left": "Constraints", "right": "Verification", "rel": "verifiedBy"},
    {"left": "What", "right": "HowType", "rel": "refines"},
    {"left": "What", "right": "Verification", "rel": "verifiedBy"},
    {"left": "Verification", "right": "AcceptanceCriteria", "rel": "acceptedBy"},
    {"left": "AcceptanceCriteria", "right": "Verification", "rel": "requires"},
    # [ì•ˆì „ì¥ì¹˜] Anchorê°€ ì—†ìœ¼ë©´ í–‰ìœ„(What)ë„ ì„±ë¦½ ë¶ˆê°€ (ê¸°ë³¸ ëŒ€ì „ì œ)
    {"left": "Anchor", "right": "What", "rel": "performs"}
]

# [ê¸°ë³¸ ë„ë©”ì¸ ì‚¬ì „] - ë‹¨ìœ„ë‚˜ í•„ìˆ˜ ìš©ì–´ (ìë™ ì¶”ì¶œë¡œ ë†“ì¹  ìˆ˜ ìˆëŠ” ê²ƒë“¤)
BASE_DOMAIN_TERMS = [
    # Units & Common
    "ms", "s", "sec", "msec", "Hz", "V", "A", "mA", "Nm", "kW", "kph", "mph", "%", "deg", "C", "bar",
    "Time", "Voltage", "Current", "Temperature", "Pressure", "Speed", "Torque",
    "ECU", "Sensor", "Actuator", "System", "Function", "Module"
]

# [ë¶ˆìš©ì–´] - ë„ë©”ì¸ ìš©ì–´ë¡œ ì˜¤í•´í•˜ê¸° ì‰¬ìš´ ì¼ë°˜ ëª…ì‚¬ë“¤
STOPWORDS = {
    "ê²½ìš°", "ë•Œ", "ìˆ˜", "ê²ƒ", "ë“±", "ë°", "í•¨", "ì „", "í›„", "ì‹œ", 
    "ì´ìƒ", "ì´í•˜", "ì´ˆê³¼", "ë¯¸ë§Œ", "ë‚´", "ê°„", "ê°’", "ì¤‘", "ìœ„", 
    "ëŒ€í•´", "ê´€ë ¨", "ì‚¬ìš©", "ìˆ˜í–‰", "ë™ì‘", "ìƒíƒœ", "ë°œìƒ", "ê¸°ëŠ¥",
    "í¬í•¨", "ì ìš©", "ìš”êµ¬", "í™•ì¸", "ì œê³µ", "ìœ ì§€", "ì„¤ì •", "ë°©ì‹",
    "ê¸°ì¤€", "í•­ëª©", "ë‚´ìš©", "ë¶€ë¶„", "ì‚¬ì´", "ë‹¤ìŒ", "ì•„ë˜", "ìœ„í•´",
    "ê°€ëŠ¥", "í•„ìš”", "ë„ë‹¬", "ê°ì§€", "íŒë‹¨", "ì—¬ë¶€", "ì§í›„", "ì´ì „",
    "ë„", "ë¶„", "ì´ˆ", "íšŒ", "ê°œ", "ë²ˆ", "ê°€ì§€" 
}

# =========================================================
# [Module 1] Domain Term Extractor (Auto-Learning)
# =========================================================
class DomainTermExtractor:
    def __init__(self):
        self.kiwi = Kiwi()
        self.term_counter = Counter()

    def _is_valid_term(self, word: str, tag: str) -> bool:
        if word in STOPWORDS: return False
        # í•œ ê¸€ì í•œê¸€ ëª…ì‚¬ëŠ” ë…¸ì´ì¦ˆê°€ ë§ìŒ (ë‹¨, ì˜ì–´ëŠ” í—ˆìš©)
        if len(word) == 1 and not re.match(r'[a-zA-Z]', word): return False
        if re.match(r'^\d+$', word): return False
        return True

    def extract(self, items: List[Dict]) -> List[str]:
        print(f"ğŸ“– Learning domain terms from {len(items)} items...")
        
        for item in items:
            # 1. ë©”íƒ€ë°ì´í„° (ê°€ì¤‘ì¹˜ ë†’ìŒ)
            meta = item.get('meta', {})
            explicit = [meta.get('component'), meta.get('ecu'), item.get('controller'), item.get('vehicle')]
            if 'vehicle_models' in meta: explicit.extend(meta['vehicle_models'])
            
            for term in explicit:
                if term and isinstance(term, str):
                    self.term_counter[term.strip()] += 10

            # 2. ë³¸ë¬¸ ë¶„ì„ (ê°€ì¤‘ì¹˜ ë³´í†µ)
            raw = item.get('raw_text', '')
            if raw:
                tokens = self.kiwi.tokenize(raw)
                for t in tokens:
                    if t.tag in ['NNG', 'NNP', 'SL']:
                        if self._is_valid_term(t.form, t.tag):
                            self.term_counter[t.form] += 1
        
        # ë¹ˆë„ 2 ì´ìƒì¸ ìš©ì–´ë§Œ ì¶”ì¶œ
        extracted = [term for term, count in self.term_counter.items() if count >= 2]
        # ë¹ˆë„ìˆœ ì •ë ¬
        extracted.sort(key=lambda x: self.term_counter[x], reverse=True)
        
        print(f"ğŸ“Š Learned {len(extracted)} domain terms (Top 5: {extracted[:5]})")
        return extracted

# =========================================================
# [Module 2] Parser Data Structures
# =========================================================
class SlotState(str, Enum):
    OK = "OK"
    WEAK = "WEAK"
    ABSENT = "ABSENT"

@dataclass
class SlotData:
    candidates: List[str] = field(default_factory=list)
    selected: Optional[str] = None
    state: SlotState = SlotState.ABSENT

@dataclass
class ParseResult:
    id: str
    raw_text: str
    mrs_type: str = "Unknown"
    type_rationale: str = ""
    slots: Dict[str, SlotData] = field(default_factory=dict)
    logs: List[str] = field(default_factory=list)

# =========================================================
# Stage 1: Kiwi Candidate Generator
# =========================================================
class KiwiCandidateGenerator:
    def __init__(self, learned_terms: List[str]):
        self.kiwi = Kiwi()
        self.domain_terms = list(set(BASE_DOMAIN_TERMS + learned_terms))
        self._register_user_words()
        
        self.regex_patterns = {
            "When": r"(if|when|upon|during|in case of|while|after|before|ì¡°ê±´|~ì‹œ|~ê²½ìš°|~ë™ì•ˆ|ë°œìƒ ì‹œ)[^,.]*",
            "Why": r"(to prevent|in order to|ensure|guarantee|ëª©ì |ìœ„í•´|ë³´ì¥|ë°©ì§€)[^,.]*"
        }

    def _register_user_words(self):
        for term in self.domain_terms:
            self.kiwi.add_user_word(term, tag='NNP', score=10)
    
    def _normalize(self, text: str) -> str:
        return text.strip()

    def generate(self, item: dict) -> ParseResult:
        raw_text = self._normalize(item.get('raw_text', ''))
        req_id = item.get('req_id', item.get('id', 'N/A'))
        result = ParseResult(id=req_id, raw_text=raw_text)
        
        tokens = self.kiwi.tokenize(raw_text)
        candidates = {k: [] for k in ["Anchor", "What", "Constraints", "Verification", "AcceptanceCriteria", "HowType", "When", "Why"]}
        
        # ë°˜ë³µë¬¸ ì œì–´ë¥¼ ìœ„í•´ while ì‚¬ìš© (í† í° ê±´ë„ˆë›°ê¸° ë“± ìœ ì—°ì„± í™•ë³´)
        i = 0
        while i < len(tokens):
            token = tokens[i]
            form, tag = token.form, token.tag
            is_consumed = False # í˜„ì¬ í† í°ì´ íŠ¹ì • ìŠ¬ë¡¯ìœ¼ë¡œ ì²˜ë¦¬ë˜ì—ˆëŠ”ì§€ ì—¬ë¶€
            
            # 1. [Constraints] ìˆ«ì(SN) + ë‹¨ìœ„
            # ìˆ˜ì¹˜ ë’¤ì— ì˜¤ëŠ” ê²ƒì´ ë‹¨ìœ„(SL, NNB, NNG)ë¼ë©´ Constraintsë¡œ ë¬¶ìŒ
            if tag == 'SN':
                phrase = form
                # ë’¤ì— ë‹¨ìœ„ê°€ ìˆëŠ”ì§€ í™•ì¸
                if i + 1 < len(tokens):
                    next_t = tokens[i+1]
                    # SL(ì˜ì–´ë‹¨ìœ„), NNB(ì˜ì¡´ëª…ì‚¬: ë²ˆ, ê°œ, ì´ˆ), NNG(ì¼ë°˜ëª…ì‚¬: ë„, ë¶„)
                    if next_t.tag in ['SL', 'NNB', 'NNG'] or next_t.form in ['ms', 's', 'V', 'A', 'ë„', 'ë¶„', 'ì´ˆ']:
                        phrase += next_t.form
                        is_consumed = True # ë’¤ í† í°ê¹Œì§€ ì†Œëª¨í–ˆë‹¤ê³  ê°€ì •í•  ìˆ˜ë„ ìˆìŒ(ì—¬ê¸°ì„  ë‹¨ìˆœ ë³‘í•©)
                
                candidates["Constraints"].append(phrase)
                # ìˆ«ìëŠ” Anchorê°€ ë  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ consumed ì²˜ë¦¬
                is_consumed = True

            # 2. [What] ë™ì‚¬(VV) ë° ëª…ì‚¬í˜• ë™ì‚¬(NNG+XSV) ì²˜ë¦¬ â˜…í•µì‹¬ ìˆ˜ì •â˜…
            
            # Case A: ëª…ì‚¬(NNG) + íŒŒìƒì ‘ë¯¸ì‚¬(XSV: í•˜, ë˜, ì‹œí‚¤) -> ëª…ì‚¬í˜• ë™ì‚¬
            # ì˜ˆ: "ê°€ì†(NNG) + í•˜(XSV)" -> "ê°€ì†í•˜(What)"
            if tag in ['NNG', 'NNP'] and i + 1 < len(tokens):
                next_t = tokens[i+1]
                if next_t.tag in ['XSV']:  # -í•˜, -ë˜, -ì‹œí‚¤ ë“±
                    # ì ‘ë¯¸ì‚¬ ë’¤ì— ì–´ë¯¸(E...)ê°€ ë¶™ìœ¼ë©´ ë” ê¸¸ê²Œ ê°€ì ¸ì˜´
                    phrase = form + next_t.form
                    lookahead = 2
                    while i + lookahead < len(tokens) and tokens[i+lookahead].tag.startswith('E'):
                        phrase += tokens[i+lookahead].form
                        lookahead += 1
                    
                    candidates["What"].append(phrase)
                    is_consumed = True # ëª…ì‚¬ì˜€ì§€ë§Œ ë™ì‚¬ë¡œ ì“°ì˜€ìœ¼ë¯€ë¡œ Anchor í›„ë³´ì—ì„œ ì œì™¸!
            
            # Case B: ëª…ì‚¬(NNG) + ë™ì‚¬ 'í•˜ë‹¤/ë˜ë‹¤'(VV) ê°€ ë¶„ë¦¬ëœ ê²½ìš°
            # ì˜ˆ: "ë™ì‘(NNG) + ì„(JKO) + í•œë‹¤(VV)" -> "ë™ì‘ì„ í•œë‹¤(What)"
            # ì˜ˆ: "ë™ì‘(NNG) + í•œë‹¤(VV)"
            if tag in ['NNG', 'NNP'] and not is_consumed:
                # ë°”ë¡œ ë’¤ë‚˜, ì¡°ì‚¬ ë’¤ì— 'í•˜ë‹¤/ë˜ë‹¤'ê°€ ì˜¤ëŠ”ì§€ ì²´í¬
                # ê°„ë‹¨íˆ ë°”ë¡œ ë’¤ì— 'í•˜ë‹¤' ê³„ì—´ì´ ì˜¤ëŠ” ê²½ìš°ë§Œ Whatìœ¼ë¡œ ë³‘í•© (ë³µì¡ì„± ë°©ì§€)
                if i + 1 < len(tokens):
                    next_t = tokens[i+1]
                    if next_t.tag == 'VV' and next_t.form in ['í•˜', 'ë˜', 'ì‹œí‚¤']:
                        phrase = form + next_t.form
                        # ì–´ë¯¸ ì¶”ê°€
                        lookahead = 2
                        while i + lookahead < len(tokens) and tokens[i+lookahead].tag.startswith('E'):
                            phrase += tokens[i+lookahead].form
                            lookahead += 1
                        candidates["What"].append(phrase)
                        is_consumed = True

            # Case C: ìˆœìˆ˜ ë™ì‚¬(VV)
            if tag == 'VV':
                # 'í•˜', 'ë˜' ê°™ì€ ë³´ì¡°ì  ë™ì‚¬ê°€ ë‹¨ë…ìœ¼ë¡œ ì“°ì¸ê²Œ ì•„ë‹ˆë¼ë©´
                if form not in ['í•˜', 'ë˜'] or (i > 0 and tokens[i-1].tag not in ['NNG', 'NNP']): 
                    phrase = form
                    lookahead = 1
                    while i + lookahead < len(tokens) and tokens[i+lookahead].tag.startswith('E'):
                        phrase += tokens[i+lookahead].form
                        lookahead += 1
                    candidates["What"].append(phrase)
                    is_consumed = True

            # 3. [Anchor] ëª…ì‚¬(NNG, NNP, SL)
            # â˜…ì¤‘ìš”â˜…: ìœ„ì—ì„œ ëª…ì‚¬í˜• ë™ì‚¬(is_consumed)ë¡œ íŒëª…ëœ ê²½ìš° Anchorì— ë„£ì§€ ì•ŠìŒ
            if tag in ['NNG', 'NNP', 'SL'] and not is_consumed:
                # ë„ë©”ì¸ ìš©ì–´ì´ê±°ë‚˜ ì¡°ì‚¬ê°€ ë¶™ì€ ê²½ìš°
                if form in self.domain_terms or (i+1 < len(tokens) and tokens[i+1].tag in ['JKS', 'JX', 'JC', 'JKO']):
                    candidates["Anchor"].append(form)

            i += 1
        
        # 4. ì •ê·œì‹ ë³´ì¡° (When, Why)
        for slot, pat in self.regex_patterns.items():
            matches = re.finditer(pat, raw_text, re.IGNORECASE)
            for m in matches:
                candidates[slot].append(m.group().strip())

        # 5. ê²°ê³¼ ì •ë¦¬
        for slot, cands in candidates.items():
            unique_cands = sorted(list(set(cands)), key=len, reverse=True)
            state = SlotState.WEAK if unique_cands else SlotState.ABSENT
            result.slots[slot] = SlotData(candidates=unique_cands, state=state)

        return result

# =========================================================
# Stage 2: LLM Selector (Enhanced Anchor Logic)
# =========================================================
class LLMSelector:
    def __init__(self, model="mistral"):
        self.model = model
        self.api_url = "http://localhost:11434/api/generate"

    def select(self, result: ParseResult) -> ParseResult:
        active_candidates = {k: v.candidates for k, v in result.slots.items() if v.candidates}
        
        if not active_candidates:
            result.logs.append("â„¹ï¸ No candidates to verify.")
            return result

        # [í•µì‹¬ ìˆ˜ì •] ì‚¬ìš©ìë‹˜ì´ ì •ì˜í•œ 3ê°€ì§€ ê¸°ì¤€ì„ í”„ë¡¬í”„íŠ¸ ê·œì¹™ìœ¼ë¡œ ë³€í™˜
        prompt = f"""
You are an expert Requirements Analyst.
I have extracted candidate keywords using a morphological analyzer (Kiwi).
Kiwi extracts all nouns, verbs, and numbers without context.
Your job is to apply **CONTEXTUAL LOGIC** to filter these candidates.

Requirement: "{result.raw_text}"
Candidates: {json.dumps(active_candidates, ensure_ascii=False)}

### âš ï¸ CRITICAL FILTERING RULES (MUST FOLLOW) âš ï¸ ###

1. **Anchor: Distinguish Subject vs. Object**
   - Kiwi captures ALL nouns. You must identify the **Active Agent** (Subject).
   - **Rule**: If 'Candidate A' acts upon 'Candidate B', then 'A' is the Anchor. 'B' is the Target/Object (Ignore B).
   - *Example*: "Diagnostic Tool checks BMS" -> Anchor: ["Diagnostic Tool"] (NOT BMS).
   - *Exception*: If the sentence is Passive ("BMS is checked"), then 'BMS' is the Anchor.

2. **Constraints: Distinguish Limit vs. ID**
   - Kiwi captures ALL numbers. You must identify **Performance Limits** (Time, Voltage, etc.).
   - **Rule**: Identifiers (CAN ID, HEX codes, Addresses, Version numbers) are **NOT** constraints.
   - *Example*: "Send CAN ID 0x100 every 10ms" -> Constraints: ["10ms"] (Ignore 0x100).

3. **What: Distinguish Main Action vs. Modifier**
   - Kiwi captures ALL verbs. You must identify the **Main Clause Action** (Shall/Must).
   - **Rule**: Ignore verbs used as adjectives, modifiers, or inside 'If/When' conditions.
   - *Example*: "Controller *detecting* error *shall shut down*" -> What: ["shall shut down"] (Ignore 'detecting' - it's a modifier/condition).

### INSTRUCTIONS ###
1. Select the BEST span(s) for each slot based on the rules above.
2. Return a **LIST** of strings for each slot.
3. If all candidates for a slot are invalid (e.g., only IDs found for Constraints), return "NONE".
4. Return JSON object: {{ "SlotName": ["SelectedSpan1", ...] }}
"""
        payload = {
            "model": self.model, "prompt": prompt, "format": "json", "stream": False,
            "options": {"temperature": 0.0} # ê²°ì •ë¡ ì  ë‹µë³€ì„ ìœ„í•´ 0.0 ìœ ì§€
        }

        try:
            resp = requests.post(self.api_url, json=payload, timeout=20)
            resp.raise_for_status()
            llm_out = json.loads(resp.json().get('response', '{}'))
            
            for slot, selection in llm_out.items():
                if slot in result.slots:
                    # ê²°ê³¼ë¥¼ ë¦¬ìŠ¤íŠ¸ë¡œ ì •ê·œí™”
                    valid_selections = []
                    raw_list = selection if isinstance(selection, list) else ([selection] if isinstance(selection, str) else [])
                    
                    for val in raw_list:
                        if val and str(val).upper() not in ["NONE", "ABSENT", "NULL", ""]:
                            valid_selections.append(val)
                    
                    if valid_selections:
                        result.slots[slot].selected = valid_selections
                        result.slots[slot].state = SlotState.OK
                    else:
                        result.slots[slot].selected = []
                        result.slots[slot].state = SlotState.ABSENT

        except Exception as e:
            result.logs.append(f"âš ï¸ LLM Error: {e}")
            
        return result
    
# =========================================================
# Stage 3: Hierarchy Validator
# =========================================================
class HierarchyValidator:
    def validate(self, result: ParseResult) -> ParseResult:
        slots = result.slots
        logs = result.logs
        
        # ê´€ê³„ ê·œì¹™ ìˆœíšŒ
        for rule in HIERARCHY_RELATIONS:
            left_key = rule['left']   # ì „ë°˜ë¶€ (í•„ìˆ˜ ì¡°ê±´)
            right_key = rule['right'] # í›„ë°˜ë¶€ (ì¢…ì† ëŒ€ìƒ)
            relation = rule.get('rel', 'related')
            
            # ë¡œì§: "í›„ë°˜ë¶€(Right)ëŠ” ìˆëŠ”ë°(OK), ì „ë°˜ë¶€(Left)ê°€ ì—†ë‹¤ë©´(ABSENT) -> ë¬¸ì œ ë°œìƒ"
            # ì¡°ì¹˜: í›„ë°˜ë¶€(Right)ë¥¼ ì‹ ë¢°í•  ìˆ˜ ì—†ìœ¼ë¯€ë¡œ ì œê±°(Prune)
            if slots[right_key].state == SlotState.OK and slots[left_key].state == SlotState.ABSENT:
                
                old_val = slots[right_key].selected
                
                # í›„ë°˜ë¶€ ìŠ¬ë¡¯ ë¬´íš¨í™”
                slots[right_key].state = SlotState.ABSENT
                slots[right_key].selected = []
                
                logs.append(f"âœ‚ï¸ [Hierarchy] Pruned '{right_key}' ({old_val}) because source '{left_key}' is missing (Relation: {relation}).")

        return result

# =========================================================
# Stage 4: Type Determiner
# =========================================================
def determine_type(result: ParseResult, config_yaml: str):
    config = yaml.safe_load(config_yaml)
    type_defs = config['mrs_schema']['mrs_only_types']
    states = {k: v.state for k, v in result.slots.items()}
    
    for t_name in type_defs['type_order']:
        criteria = type_defs['types'][t_name]['match']
        match = True
        reasons = []
        for cond in criteria.get('all', []):
            cur = states.get(cond['slot'], SlotState.ABSENT)
            if cur.name not in cond['state_in']:
                match = False; break
            reasons.append(f"{cond['slot']}={cur.name}")
        
        if match:
            result.mrs_type = t_name
            result.type_rationale = ", ".join(reasons)
            return

# =========================================================
# Main Execution Flow
# =========================================================
def run_kiwi_pipeline():
    # 0. Data Load (ì „ì²´ ë°ì´í„°ë¥¼ ë¨¼ì € ë¡œë“œí•´ì•¼ í•™ìŠµ ê°€ëŠ¥)
    items = []
    files = glob.glob(os.path.join('./data/', "*.json"))
    if not files and os.path.exists("FuSaReq_new_augmented.json"): files = ["FuSaReq_new_augmented.json"]
    
    for jf in files:
        with open(jf, 'r', encoding='utf-8') as f:
            c = json.load(f)
            if isinstance(c, list): items.extend(c)
            elif isinstance(c, dict) and 'requirements' in c: items.extend(c['requirements'])

    if not items:
        print("âŒ No data found.")
        return

    # 1. Domain Term Learning (Extractor ì‹¤í–‰)
    print("ğŸ§  [Step 1] Extracting domain terms from data...")
    extractor = DomainTermExtractor()
    learned_terms = extractor.extract(items)

    # 2. Pipeline Initialization (í•™ìŠµëœ ìš©ì–´ ì „ë‹¬)
    print("â³ [Step 2] Initializing Parser with learned terms...")
    generator = KiwiCandidateGenerator(learned_terms)
    selector = LLMSelector(model="mistral")
    validator = HierarchyValidator()

    print(f"\nğŸš€ [Step 3] Running MRS Kiwi-Hybrid Parser")
    print(f"   Flow: TermLearn -> Kiwi(Morph) -> LLM(Select) -> Logic(Hierarchy)")
    print(f"   Total Requirements: {len(items)}\n")

    for idx, item in enumerate(items, 1):
        # 3.1 Generate
        res = generator.generate(item)
        # 3.2 Select
        res = selector.select(res)
        # 3.3 Validate
        res = validator.validate(res)
        # 3.4 Type
        determine_type(res, DEFAULT_MRS_CONFIG)

        # Output
        print("\n" + "="*80)
        print(f"ğŸ”¸ [{res.id}] {res.mrs_type} (Reason: {res.type_rationale})")
        print(f"   \"{res.raw_text}\"")
        print("-" * 80)
        
        for slot, data in res.slots.items():
            if data.candidates:
                icon = "âœ…" if data.state == SlotState.OK else "â¬œ"
                if data.selected:
                    sel_text = str(data.selected) # ['BMS', 'VCU'] ì²˜ëŸ¼ ì¶œë ¥ë¨
                else:
                    sel_text = "(NONE)"
                cands_disp = str(data.candidates[:3]) + ("..." if len(data.candidates)>3 else "")
                print(f"   {icon} {slot:<12} | Selected: {sel_text:<20} | Candidates: {cands_disp}")
        
        if res.logs:
            print(f"   ğŸ“ Logs:")
            for log in res.logs: print(f"      {log}")

if __name__ == "__main__":
    run_kiwi_pipeline()